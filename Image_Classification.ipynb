{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "for i in range(1,6):\n",
    "#     path = 'cifar-10-python\\cifar-10-batches-py\\data_batch_' + str(i)\n",
    "    with open(r'cifar-10-python\\cifar-10-batches-py\\data_batch_' + str(i), mode='rb') as file:\n",
    "        # note the encoding type is 'latin1'\n",
    "        batch = pickle.load(file, encoding='latin1')\n",
    "    if i == 1:  \n",
    "        train_X = (batch['data'].reshape((len(batch['data']), 3, 32, 32)).transpose(0, 2, 3, 1)).astype('float32')\n",
    "        train_Y = batch['labels']\n",
    "    else:\n",
    "        x_train_temp = (batch['data'].reshape((len(batch['data']), 3, 32, 32)).transpose(0, 2, 3, 1)).astype('float32')\n",
    "        y_train_temp = batch['labels']\n",
    "        train_X = np.concatenate((train_X,x_train_temp),axis = 0)\n",
    "        train_Y = np.concatenate((train_Y,y_train_temp),axis=0)\n",
    "\n",
    "#path = 'cifar-10-python\\cifar-10-batches-py\\test_batch'\n",
    "with open(r'cifar-10-python\\cifar-10-batches-py\\test_batch','rb') as file:\n",
    "    # note the encoding type is 'latin1'\n",
    "    batch = pickle.load(file, encoding='latin1')\n",
    "    test_X = (batch['data'].reshape((len(batch['data']), 3, 32, 32)).transpose(0, 2, 3, 1)).astype('float32')\n",
    "    test_Y = batch['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 32, 32, 3)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Flatten\n",
    "from keras.constraints import maxnorm\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.utils import np_utils\n",
    "from keras.layers import BatchNormalization\n",
    "from keras import regularizers\n",
    "from keras.layers import Activation\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x=train_X.astype('float32')\n",
    "test_X=test_X.astype('float32')\n",
    " \n",
    "train_X=train_X/255.0\n",
    "test_X=test_X/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_Y=np_utils.to_categorical(train_Y)\n",
    "test_Y=np_utils.to_categorical(test_Y)\n",
    " \n",
    "num_classes=test_Y.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# l1 Regularization is that we have to penalize our weights by \n",
    "#adding absolute values of weight in our loss function\n",
    "\n",
    "### kernel_regularizer='l1' on the layers\n",
    "\n",
    "###\n",
    "\n",
    "#2 Regularization is another regularization technique which is \n",
    "#also known as Ridge regularization. In L2 regularization we add \n",
    "#the squared magnitude of weights to penalize our lost function.\n",
    "\n",
    "####kernel_regularizer='l2'  on the layers\n",
    "\n",
    "####\n",
    "\n",
    "###dropout The main idea behind using dropout is that we randomly \n",
    "#turn off some neurons in our layer based on some probability.\n",
    "\n",
    "### Dropout(0.5) add a layer \n",
    "\n",
    "###\n",
    "\n",
    "###batch normalization he main idea behind batch normalization is \n",
    "#that we normalize the input layer by using several techniques\n",
    "\n",
    "### BatchNormalization() add a layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### explanation\n",
    "\n",
    "model=Sequential() #create empty sequential model and then add layers\n",
    "\n",
    "#Layer 1: conv layer, filter size 3X3, stride size 1 in both dimensions,\n",
    "#depth 32. Padding same and activation relu will apply to all layers.\n",
    "#we will use ReLU activation for all our layers, except for the last layer\n",
    "#remember that ReLU doesn't map negative values (no negative values here)\n",
    "#no specification of stride default setting = 1\n",
    "#input shape needs to be specified, but not for the following layers\n",
    "model.add(Conv2D(32,(3,3), activation='relu', padding='same',\\\n",
    "                 input_shape=(32,32,3)))\n",
    "\n",
    "#Layer 2: conv layer, filter size 3X3, stride size 1 in both dimensions,\n",
    "#depth 32. Padding same and activation relu will apply to all layers.\n",
    "#we would need padding 1 to achieve the same width and height, but \n",
    "#we will use 'same' padding for all the conv layers, aka zero pad.\n",
    "model.add(Conv2D(32,(3,3), activation='relu', padding='same'))\n",
    "    \n",
    "#Layer 3: max pooling layer, pool size 2X2, stride 2 in both dimensions,\n",
    "#max pooling layer stride default given by pool size\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "#Layer 4: dropout layer with probability 25% of dropout, to prevent overfitting\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "#Layers 5-8: same but depth of conv layer is 64 instead of 32\n",
    "model.add(Conv2D(64,(3,3), activation='relu', padding='same'))\n",
    "model.add(Conv2D(64,(3,3), activation='relu', padding='same'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "#Layer 9: FC (Fully-Connected) layer. Now our neurons are not just in \n",
    "#on row, but spatially arranged in a cube-like format. We need to make\n",
    "#them into one row, flattening. Flatten layer.\n",
    "model.add(Flatten()) #now one row\n",
    "model.add(Dense(512,activation='relu')) #dense (FC) layer. \n",
    "\n",
    "#Layer 10: another dropout of probability 50%\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "#Layers 11-12: another dense (FC) layer with 10 neurons and sofrmax activation\n",
    "#last layer, softmax, only transforms the output of the previous layer \n",
    "#into probability distributions, which is the final goal\n",
    "model.add(Dense(10,activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## layers 1: original ==best one\n",
    "model=Sequential()\n",
    "model.add(Conv2D(32,(3,3),input_shape=(32,32,3),\n",
    "    padding='same',activation='relu',\n",
    "    kernel_constraint=maxnorm(3)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Conv2D(32,(3,3),activation='relu',padding='same',kernel_constraint=maxnorm(3)))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512,activation='relu',kernel_constraint=maxnorm(3)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "## layers 2: original + batch normalization == a lot overfitting\n",
    "model=Sequential()\n",
    "model.add(Conv2D(32,(3,3),input_shape=(32,32,3),\n",
    "    padding='same',activation='relu',\n",
    "    kernel_constraint=maxnorm(3)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(32,(3,3),activation='relu',padding='same',kernel_constraint=maxnorm(3)))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Flatten())\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(512,activation='relu',kernel_constraint=maxnorm(3)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(num_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## layers 3: batch normalization remove batch size == a lot overfitting\n",
    "\n",
    "\n",
    "\n",
    "model=Sequential()\n",
    "\n",
    "model.add(Conv2D(32,(3,3),input_shape=(32,32,3),\n",
    "    padding='same',activation='relu',\n",
    "    kernel_constraint=maxnorm(3)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(32,(3,3),activation='relu',padding='same',kernel_constraint=maxnorm(3)))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512,activation='relu',kernel_constraint=maxnorm(3)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(512,activation='relu',kernel_constraint=maxnorm(3)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(512,activation='relu',kernel_constraint=maxnorm(3)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512,activation='relu',kernel_constraint=maxnorm(3)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(512,activation='relu',kernel_constraint=maxnorm(3)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(512,activation='relu',kernel_constraint=maxnorm(3)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512,activation='relu',kernel_constraint=maxnorm(3)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(512,activation='relu',kernel_constraint=maxnorm(3)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(512,activation='relu',kernel_constraint=maxnorm(3)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(num_classes, activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "### layers 4:   epoch50 batch64 good? \n",
    "\n",
    "\n",
    "weight_decay = 1e-4\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(filters = 32, kernel_size = (3,3), padding='same',input_shape=(32,32,3), kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(filters = 32, kernel_size = (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(2,2))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Conv2D(filters = 64, kernel_size = (3,3), padding='same',input_shape=(32,32,3), kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(filters = 64, kernel_size = (3,3), padding='same', input_shape=(32,32,3),kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(2,2))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Conv2D(filters = 128, kernel_size = (3,3), padding='same',input_shape=(32,32,3), kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(filters = 128, kernel_size = (3,3), padding='same', input_shape=(32,32,3),kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(2,2))\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(units = 10, activation = 'softmax'))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "### layers 5:  epoch100 batch32 == need testing\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(32, (3, 3), padding='same',\n",
    "                 input_shape=(32,32,3)))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Conv2D(48, (3, 3), padding='same',\n",
    "                input_shape=(32,32,3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(80, (3, 3), padding='same',\n",
    "                 input_shape=(32,32,3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(128, (3, 3), padding='same',\n",
    "                input_shape=(32,32,3)))\n",
    "model.add(MaxPooling2D())\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "\n",
    "model.add(Dense(500))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(num_classes,activation='relu'))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd=SGD(lr=0.1,momentum=0.9,decay=(0.01/25),nesterov=False)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',optimizer=sgd,metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam=Adam(learning_rate=0.01,decay = 1e-6,amsgrad=False)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',optimizer=adam,metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_12 (Conv2D)          (None, 32, 32, 32)        896       \n",
      "                                                                 \n",
      " activation_10 (Activation)  (None, 32, 32, 32)        0         \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 32, 32, 32)       128       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " conv2d_13 (Conv2D)          (None, 32, 32, 32)        9248      \n",
      "                                                                 \n",
      " activation_11 (Activation)  (None, 32, 32, 32)        0         \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 32, 32, 32)       128       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_8 (MaxPooling  (None, 16, 16, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_9 (Dropout)         (None, 16, 16, 32)        0         \n",
      "                                                                 \n",
      " conv2d_14 (Conv2D)          (None, 16, 16, 64)        18496     \n",
      "                                                                 \n",
      " activation_12 (Activation)  (None, 16, 16, 64)        0         \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 16, 16, 64)       256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_15 (Conv2D)          (None, 16, 16, 64)        36928     \n",
      "                                                                 \n",
      " activation_13 (Activation)  (None, 16, 16, 64)        0         \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 16, 16, 64)       256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_9 (MaxPooling  (None, 8, 8, 64)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_10 (Dropout)        (None, 8, 8, 64)          0         \n",
      "                                                                 \n",
      " conv2d_16 (Conv2D)          (None, 8, 8, 128)         73856     \n",
      "                                                                 \n",
      " activation_14 (Activation)  (None, 8, 8, 128)         0         \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 8, 8, 128)        512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_17 (Conv2D)          (None, 8, 8, 128)         147584    \n",
      "                                                                 \n",
      " activation_15 (Activation)  (None, 8, 8, 128)         0         \n",
      "                                                                 \n",
      " batch_normalization_5 (Batc  (None, 8, 8, 128)        512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_10 (MaxPoolin  (None, 4, 4, 128)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_11 (Dropout)        (None, 4, 4, 128)         0         \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 2048)              0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 10)                20490     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 309,290\n",
      "Trainable params: 308,394\n",
      "Non-trainable params: 896\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "782/782 [==============================] - 155s 197ms/step - loss: 2.0910 - accuracy: 0.4226 - val_loss: 1.5674 - val_accuracy: 0.4973\n",
      "Epoch 2/15\n",
      "782/782 [==============================] - 155s 198ms/step - loss: 1.2795 - accuracy: 0.6180 - val_loss: 1.1842 - val_accuracy: 0.6717\n",
      "Epoch 3/15\n",
      "782/782 [==============================] - 160s 205ms/step - loss: 1.2327 - accuracy: 0.6707 - val_loss: 1.2749 - val_accuracy: 0.6701\n",
      "Epoch 4/15\n",
      "782/782 [==============================] - 167s 213ms/step - loss: 1.2403 - accuracy: 0.6869 - val_loss: 1.5648 - val_accuracy: 0.5852\n",
      "Epoch 5/15\n",
      "782/782 [==============================] - 165s 211ms/step - loss: 1.2585 - accuracy: 0.6932 - val_loss: 1.3551 - val_accuracy: 0.6559\n",
      "Epoch 6/15\n",
      "782/782 [==============================] - 158s 202ms/step - loss: 1.2540 - accuracy: 0.7003 - val_loss: 1.2404 - val_accuracy: 0.7023\n",
      "Epoch 7/15\n",
      "782/782 [==============================] - 156s 199ms/step - loss: 1.2361 - accuracy: 0.7041 - val_loss: 1.1981 - val_accuracy: 0.7174\n",
      "Epoch 8/15\n",
      "782/782 [==============================] - 157s 201ms/step - loss: 1.2266 - accuracy: 0.7085 - val_loss: 1.3141 - val_accuracy: 0.6778\n",
      "Epoch 9/15\n",
      "782/782 [==============================] - 156s 200ms/step - loss: 1.2187 - accuracy: 0.7119 - val_loss: 1.5145 - val_accuracy: 0.6249\n",
      "Epoch 10/15\n",
      "782/782 [==============================] - 163s 208ms/step - loss: 1.2168 - accuracy: 0.7137 - val_loss: 1.2415 - val_accuracy: 0.7069\n",
      "Epoch 11/15\n",
      "782/782 [==============================] - 151s 194ms/step - loss: 1.2047 - accuracy: 0.7176 - val_loss: 1.2152 - val_accuracy: 0.7147\n",
      "Epoch 12/15\n",
      "782/782 [==============================] - 151s 193ms/step - loss: 1.2039 - accuracy: 0.7187 - val_loss: 1.1464 - val_accuracy: 0.7317\n",
      "Epoch 13/15\n",
      "782/782 [==============================] - 154s 197ms/step - loss: 1.1978 - accuracy: 0.7166 - val_loss: 1.5570 - val_accuracy: 0.6099\n",
      "Epoch 14/15\n",
      "782/782 [==============================] - 154s 197ms/step - loss: 1.1847 - accuracy: 0.7213 - val_loss: 1.1744 - val_accuracy: 0.7346\n",
      "Epoch 15/15\n",
      "782/782 [==============================] - 160s 204ms/step - loss: 1.1854 - accuracy: 0.7232 - val_loss: 1.3200 - val_accuracy: 0.6860\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x169bf537a30>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_X,train_Y,validation_data=(test_X,test_Y),epochs=15,batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1563/1563 [==============================] - 55s 35ms/step - loss: 1.2471 - accuracy: 0.7081\n",
      "Loss is 1.2470686435699463,\n",
      "Accuracy is 70.80600261688232\n",
      "313/313 [==============================] - 11s 35ms/step - loss: 1.3200 - accuracy: 0.6860\n",
      "Loss is 1.319950819015503,\n",
      "Accuracy is 68.59999895095825\n"
     ]
    }
   ],
   "source": [
    "\n",
    "loss8, acc8 =model.evaluate(train_X,train_Y)\n",
    "print(f\"Loss is {loss8},\\nAccuracy is {acc8 * 100}\")\n",
    "loss8, acc8 = model.evaluate(test_X, test_Y)\n",
    "print(f\"Loss is {loss8},\\nAccuracy is {acc8 * 100}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### batch normalization epoch = 10, Loss is 1.2, acc=65.71\n",
    "### original epoch = 15,bs = 32 Loss=0.85 , acc=70.89\n",
    "###batch normalization epoch = 50,bs = 40 Loss=0.93 , acc=72.74\n",
    "###batch normalization epoch = 100,bs = 32 Loss=0.84 , acc=74.29\n",
    "###layer 4 acc=74,80\n",
    "### layer 5 epoch = 25,bs = 64 Loss=1.31 , acc=68.59"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"l5e25b64.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import filedialog\n",
    "from tkinter import *\n",
    "from PIL import ImageTk, Image\n",
    "import numpy\n",
    "#load the trained model to classify the images\n",
    "from keras.models import load_model\n",
    "model = load_model('bne100b32.h5')\n",
    "#dictionary to label all the CIFAR-10 dataset classes.\n",
    "classes = { \n",
    "    0:'Avião',\n",
    "   1:'Carro',\n",
    "   2:'Passaro',\n",
    "   3:'Gato',\n",
    "   4:'Cervo',\n",
    "   5:'Cachorro',\n",
    "   6:'Sapo',\n",
    "   7:'Cavalo',\n",
    "   8:'Navio',\n",
    "   9:'Caminhão'\n",
    "}\n",
    "#initialise GUI\n",
    "top=tk.Tk()\n",
    "top.geometry('800x600')\n",
    "top.title('Image Classification CIFAR10')\n",
    "top.configure(background='#CDCDCD')\n",
    "label=Label(top,background='#CDCDCD', font=('arial',15,'bold'))\n",
    "sign_image = Label(top)\n",
    "def classify(file_path):\n",
    "    global label_packed\n",
    "    image = Image.open(file_path)\n",
    "    image = image.resize((32,32))\n",
    "    image = numpy.expand_dims(image, axis=0)\n",
    "    image = numpy.array(image)\n",
    "    pred = np.argmax(model.predict([image])[0],axis=-1)\n",
    "    sign = classes[pred]\n",
    "#     print(sign)\n",
    "    label.configure(foreground='#011638', text=sign) \n",
    "def show_classify_button(file_path):\n",
    "    classify_b=Button(top,text=\"Classify Image\",\n",
    "   command=lambda: classify(file_path),padx=10,pady=5)\n",
    "    classify_b.configure(background='#364156', foreground='white',\n",
    "font=('arial',10,'bold'))\n",
    "    classify_b.place(relx=0.79,rely=0.46)\n",
    "def upload_image():\n",
    "    try:\n",
    "        file_path=filedialog.askopenfilename()\n",
    "        uploaded=Image.open(file_path)\n",
    "        uploaded.thumbnail(((top.winfo_width()/2.25),\n",
    "        (top.winfo_height()/2.25)))\n",
    "        im=ImageTk.PhotoImage(uploaded)\n",
    "        sign_image.configure(image=im)\n",
    "        sign_image.image=im\n",
    "        label.configure(text='')\n",
    "        show_classify_button(file_path)\n",
    "    except:\n",
    "        pass\n",
    "upload=Button(top,text=\"Upload an image\",command=upload_image,padx=10,pady=5)\n",
    "upload.configure(background='#364156', foreground='white',font=('arial',10,'bold'))\n",
    "upload.pack(side=BOTTOM,pady=50)\n",
    "sign_image.pack(side=BOTTOM,expand=True)\n",
    "label.pack(side=BOTTOM,expand=True)\n",
    "heading = Label(top, text=\"Image Classification CIFAR10\",pady=20, font=('arial',20,'bold'))\n",
    "heading.configure(background='#CDCDCD',foreground='#364156')\n",
    "heading.pack()\n",
    "top.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
